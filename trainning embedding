#include "stdafx.h"
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <iostream>
#include <fstream>
#include <vector>
#include <algorithm>
#include <windows.h>
#include <process.h>
#include <time.h>
#include <iomanip>  
using namespace std;

#define MAX_STRING 100
#define EXP_TABLE_SIZE 1000
#define MAX_EXP 6
#define MAX_SENTENCE_LENGTH 20000
#define MAX_CODE_LENGTH 40
const int vocab_hash_size = 30000000;  // Maximum 30 * 0.7 = 21M words in the vocabulary
typedef float real;                    // Precision of float numbers


//抽取单词表，文本转换为数字：
char data_file[100];
char vocab_file[100];
char label_file[100];
char num_file[100]; 
char vector_file[100]; 

struct word_data{
	char *word;
	int sum;
};
word_data *vocablist; 
word_data *labellist;
long  vocab_length;
long  label_length;
long *vocab_hash;

struct syn_data{
	int label_length;
	float *vec;
};
syn_data *vec_label;
syn_data *vec_context;
long layer1_size;

float *expTable;   
int *table, key_tablesize; 
const int table_size = 1e8;

int  window;
int negative;
long long train_words = 0, word_count_actual = 0, iter, file_size = 0;
float alpha, starting_alpha, sample;

clock_t start;


// Returns position of a word in the vocabulary; if the word is not found, returns -1 
// Used later for sorting by word counts

void InitUnigramTable() {
	int a, i;
	long long train_words_pow = 0;
	real d1, power = 0.75;
	table = (int *)malloc(table_size * sizeof(int));
	for (a = 0; a < vocab_length; a++) train_words_pow += pow(vocablist[a].sum, power);
	i = 0;
	d1 = pow(vocablist[i].sum, power) / (real)train_words_pow;
	for (a = 0; a < table_size; a++) {
		table[a] = i;
		if (a / (real)table_size > d1) {
			i++;
			d1 += pow(vocablist[i].sum, power) / (real)train_words_pow;
		}
		if (i >= vocab_length) i = vocab_length - 1;
	}
}
//读入词汇表，读入文档，读入关键词表；
void Parameter_Set(  ){ 
	alpha    = 0.05;  
	starting_alpha = alpha;
	window   = 5; 
	sample   = 0;//1e-4;
	negative = 5;
	iter     = 30;
	layer1_size = 100;	
	start = clock();
	
	strcpy(vocab_file, "train_vocablist.txt");
	strcpy(label_file, "train_labellist.txt");
	strcpy(data_file,  "train_num.txt");
	strcpy(vector_file,  "vector_data.txt");
  /* 
	strcpy(vocab_file, "F:\\带标签的词向量\\上传代码\\20ng\\train_vocablist.txt");
	strcpy(label_file, "F:\\带标签的词向量\\上传代码\\20ng\\train_labellist.txt");
	strcpy(data_file,  "F:\\带标签的词向量\\上传代码\\20ng\\train_num.txt");
	strcpy(vector_file,  "F:\\带标签的词向量\\上传代码\\20ng\\vector_data.txt");
	*///cout<< data_file<<endl;
	int i;
	vocab_hash = (long *)calloc(vocab_hash_size, sizeof(int));
	expTable = (real *)malloc((EXP_TABLE_SIZE + 1) * sizeof(real));
	for (i = 0; i < EXP_TABLE_SIZE; i++){
		expTable[i] = exp((i / (real)EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); 
		expTable[i] = expTable[i] / (expTable[i] + 1);  
	}  
}
//创建in向量，创建out向量_带标签；
int Init_Vector(){
	long long a, b, i, wordsum, sum;
	unsigned long long next_random = 1;
	char word[100];
	fstream fvocab, flabel;

	fvocab.open(vocab_file,ios::in);
	fvocab >> vocab_length;
	//cout << vocab_length <<"  " << fvocab << endl;
	vocablist = new word_data[vocab_length];
	wordsum = 0;
	train_words = 0; 
    while(!fvocab.eof()){
		fvocab >> word >> sum;
		if(fvocab.eof()) break;
		vocablist[wordsum].word = new char[strlen(word)+1];
		strcpy(vocablist[wordsum].word, word);
		vocablist[wordsum].sum = sum;
		train_words += sum;
		wordsum ++;
	}

	flabel.open(label_file,ios::in);
	labellist = new word_data[100];
	label_length = 0;
	while(!flabel.eof()){
		flabel >> word >> sum;
		if(flabel.eof()) break;
		labellist[label_length].word  = new char[strlen(word) + 1];
		strcpy(labellist[label_length].word ,word);
		labellist[label_length].sum = sum;
		label_length ++;
	}

    vec_context = new syn_data[vocab_length];
    for(a = 0; a < vocab_length; a ++){ 
		vec_context[a].vec = new float[layer1_size];
		vec_context[a].label_length = 1;
	}
	for (a = 0; a < vocab_length; a ++){
		for (b = 0; b < layer1_size; b ++) {
			next_random = next_random * (unsigned long long)25214903917 + 11;
			vec_context[a].vec[b] = (((next_random & 0xFFFF) / (real)65536) - 0.5) / layer1_size;
		}
	}

	vec_label=new syn_data[vocab_length];
    for(a = 0; a < vocab_length; a ++){	 
		vec_label[a].vec = new float[layer1_size * label_length];
		vec_label[a].label_length = label_length;
	    for(i = 0; i < vec_label[a].label_length * layer1_size; i ++)  vec_label[a].vec[i]=0;
	}

	return 1;
}
//学习词向量，在vec_context上负采样；vec_label不采样，标签在out层上；
void Training_Vector() {
  long a, b, d, cw, word, last_word, sentence_length = 0, start_mark, sentence_position = 0;
  long word_count = 0, last_word_count = 0, sen[MAX_SENTENCE_LENGTH + 1];
  long l1, l2, l3, l4, c, target, label, local_iter = iter;
  unsigned long next_random = 0;
  int labels,uk,docid,keywordid,lei_sum;
  float u; 
  fstream fi;

  real f, g;
  clock_t now;
  real *neu1 = (real *)calloc(layer1_size, sizeof(real));
  real *neu1e = (real *)calloc(layer1_size, sizeof(real));
 

  docid=0;
  fi.open(data_file, ios::in);

  while (1) {	 
    if (word_count - last_word_count > 10000) {
		word_count_actual += word_count - last_word_count;
		last_word_count = word_count;      
		now = clock();
		printf("%cAlpha: %f  Progress: %.2f%%  Words/thread/sec: %.2fk  ", 13, alpha,
		word_count_actual / (real)(iter * train_words + 1) * 100,
		word_count_actual / ((real)(now - start + 1) / (real)CLOCKS_PER_SEC * 1000));
		fflush(stdout);      
		alpha = starting_alpha * (1 - word_count_actual / (real)(iter * train_words + 1));
		if (alpha < starting_alpha * 0.0001) alpha = starting_alpha * 0.0001;
    }
    if (sentence_length == 0) {
		docid ++;
		start_mark = 0;
        while (1) {
			fi>>word;
			//cout<<sentence_length<<endl;
			if (fi.eof()) break;
			if (word == -1) break;
			if (start_mark == 0){
				labels = word;
				//cout << "label = " << labels <<endl;
				start_mark =1;
				continue;
			}
			word_count ++;

			sen[sentence_length] = word;
			sentence_length ++;
		}
		//cout<<sentence_length<<endl;
	 
		sentence_position = 0;
		//cout<<docid<<endl;
    }
    if (fi.eof()) {
		word_count_actual += word_count - last_word_count;
		local_iter --;
		if (local_iter == 0) break;
		word_count = 0;
		last_word_count = 0;
		sentence_length = 0;
		fi.close( );
		fi.open(data_file,ios::in); 
		continue;
    }

	//skip samping
	word = sen[sentence_position];
	if (word == -1) continue;
	for (c = 0; c < layer1_size; c++) neu1[c] = 0;
	for (c = 0; c < layer1_size; c++) neu1e[c] = 0;
	next_random = next_random * (unsigned long long)25214903917 + 11;
	b = next_random % window;
	l4 = labels * layer1_size; 

	for (a = - b; a <= b; a ++){
		//if(vec_label[word].vecsum==1)continue;
		//if (a == window ) continue;
		c = sentence_position + a;
		if (c < 0) continue;
		if (c >= sentence_length) continue;
		last_word = sen[c];
		if (last_word == -1) continue;
		
		for (c = 0; c < layer1_size; c++) neu1e[c] = 0;
		// NEGATIVE SAMPLING to target:
		for (d = 0; d < negative + 1; d ++) {
			if (d == 0) {
				target = last_word;
				label = 1;
			} else { 
				next_random = next_random * (unsigned long long)25214903917 + 11;
				target = table[(next_random >> 16) % table_size];
				if (target == 0) target = next_random % (vocab_length - 1) + 1;
				if (target ==last_word) continue;
				label = 0; 
			}
			f = 0;
			//cout << target << " " << word << " "<< labels << " " << l4 << " " << layer1_size << endl;
			for (c = 0; c < layer1_size; c ++) f += vec_context[target].vec[c]  * vec_label[word].vec[c + l4];
			
			if (f > MAX_EXP) g = (label - 1) * alpha;
			else if (f < -MAX_EXP) g = (label - 0) * alpha;			
			else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;
			
			for (c = 0; c < layer1_size; c ++) neu1e[c] += g * vec_context[target].vec[c]; 
			for (c = 0; c < layer1_size; c ++) vec_context[target].vec[c] += g * vec_label[word].vec[c + l4];
		}
		// Learn weights input -> hidden
		for (c = 0; c < layer1_size; c ++)  vec_label[word].vec[c + l4] += neu1e[c];
	}

    sentence_position++;
    if (sentence_position >= sentence_length) {
      sentence_length = 0;
      continue;
    }
  }
  fi.close();
  free(neu1);
  free(neu1e); 
} 
//创建in向量，创建out向量_带标签；
void Save_Data(){
	long a, b, c, d,i,l4;
	fstream fo;
	float s;
	char wordname[100];
	// Save the word vectors
	fo.open(vector_file, ios::out);
	fo << vocab_length << " " << layer1_size<<endl;
	for (a = 0; a < vocab_length; a ++) {
		strcpy(wordname, vocablist[a].word); 		 
		for(i = 0; i < label_length; i ++){
			fo << wordname << " " << i << " ";
			l4 = i * layer1_size;
			for (b = 0; b < layer1_size; b ++) fo << vec_label[a].vec[l4 + b] << " ";
			fo << endl;
		}
	} 
	fo << "&*&*&" << endl;
	fo << vocab_length << " " << layer1_size << endl;

	for (a = 0; a < vocab_length; a ++) {
		fo << vocablist[a].word << " ";
		for (b = 0; b < layer1_size; b ++) fo << vec_context[a].vec[b] << " ";
		fo << endl;			
	}
	fo.close();
}


int _tmain(int argc, char** argv){ 
	Parameter_Set();
	Init_Vector( );
	InitUnigramTable( );
	Training_Vector();
	Save_Data();
	return 0;
}
